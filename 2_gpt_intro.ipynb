{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNGZVi5mIQDi4dKYgwf150S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtajeong/ChatGPT_for_Management/blob/main/2_gpt_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT (Generative Pre-trained Transformer)\n",
        "- 정의: 텍스트 데이터를 학습하여 자연어를 생성하고 이해할 수 있는 인공지능 모델\n",
        "- 훈련 데이터는 인터넷에서 크롤링한 텍스트(예: Wikipedia, 책, 웹 문서 등)와 추가적으로 대화 스타일 데이터로 구성\n",
        "- 주요 기능:\n",
        "  - 질문에 답변\n",
        "  - 텍스트 생성 (예: 이메일 작성, 보고서 작성)\n",
        "  - 번역\n",
        "  - 요약\n",
        "  - 데이터 분석 보조, 등\n",
        "- 동작 방식:\n",
        "  - Transformer 구조 (decoder 부분)\n",
        "    - 입력 텍스트 처리: GPT는 단어를 숫자로 변환해 이해(예: \"Hello\" → 숫자 배열).\n",
        "    - Attention 메커니즘: 단어 간의 관계를 이해. 예를 들어, \"The bank\"라는 문장에서 \"bank\"가 물리적 은행인지 강변(bank)인지 문맥을 통해 구별.\n",
        "    - 출력 생성: 입력 데이터를 기반으로 가장 적합한 단어를 예측해 텍스트를 생성.\n",
        "  - 사전 학습 (Pre-training)과 미세 조정 (Fine-tuning):\n",
        "    - 사전 학습: 인터넷의 방대한 데이터를 읽고 언어 패턴을 학습.\n",
        "    - 미세 조정: 특정 작업(예: 금융 데이터 분석)에 맞게 모델을 추가 학습\n"
      ],
      "metadata": {
        "id": "Cy2ea_l_6Iyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 인공지능 모델(e.g. GPT) 훈련 과정\n",
        "- 사전 훈련(Pre-training):\n",
        "  - GPT는 방대한 인터넷 데이터를 사용하여 언어 패턴을 학습\n",
        "  - 목표: 문맥을 이해하고 다음 단어를 예측하는 능력을 학습.\n",
        "  - 예: \"The stock market is\" → \"rising\" (다음 단어를 예측)\n",
        "- 미세 조정(Fine-tuning):\n",
        "  - 특정 업무(예: 고객 이메일 작성)에 맞게 모델을 추가 학습.\n",
        "  - 목표: 사전 훈련된 모델을 원하는 업무에 최적화.\n",
        "  - 예: \"Write a polite apology for delayed shipping\" → 완성된 이메일 초안 작성."
      ],
      "metadata": {
        "id": "-eKuqy_wCwhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 간단한 훈련 예시 1 (단어 예측)\n",
        "- 훈련 과정을 학생들이 직접 경험할 수 있도록 텍스트 데이터를 생성하고, 미니 모델을 훈련시키는 간단한 과정을 보인다."
      ],
      "metadata": {
        "id": "gRYJQrFUG9Or"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXXhDAEz57VM",
        "outputId": "8e8a27da-c06e-44b8-d948-4276ecdaaa88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 1, 3, 4, 5, 6, 7, 8, 1, 9, 10, 11]\n",
            "{'stock': 1, 'the': 2, 'market': 3, 'is': 4, 'unpredictable': 5, 'but': 6, 'has': 7, 'trends': 8, 'trading': 9, 'requires': 10, 'analysis': 11}\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# 훈련 데이터 (간단한 텍스트)\n",
        "data = \"The stock market is unpredictable but has trends. Stock trading requires analysis.\"\n",
        "\n",
        "# 1. 토큰화 및 데이터 전처리\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])  # 단어를 추출하고 고유한 인덱스를 부여 (index 1 부터 부여)\n",
        "                                # 인덱스 0 은 padding 용으로 예약되어 있음\n",
        "sequence = tokenizer.texts_to_sequences([data])[0]\n",
        "print(sequence)\n",
        "print(tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "sequences = []\n",
        "for i in range(1, len(sequence)):\n",
        "    sequences.append(sequence[:i+1])\n",
        "\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EzFGC70Jeeg",
        "outputId": "85b83294-540a-4fb3-f111-9c075d6824f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 1],\n",
              " [2, 1, 3],\n",
              " [2, 1, 3, 4],\n",
              " [2, 1, 3, 4, 5],\n",
              " [2, 1, 3, 4, 5, 6],\n",
              " [2, 1, 3, 4, 5, 6, 7],\n",
              " [2, 1, 3, 4, 5, 6, 7, 8],\n",
              " [2, 1, 3, 4, 5, 6, 7, 8, 1],\n",
              " [2, 1, 3, 4, 5, 6, 7, 8, 1, 9],\n",
              " [2, 1, 3, 4, 5, 6, 7, 8, 1, 9, 10],\n",
              " [2, 1, 3, 4, 5, 6, 7, 8, 1, 9, 10, 11]]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력(X)과 출력(Y) 분리\n",
        "X, Y = [], []\n",
        "for seq in sequences:\n",
        "    X.append(seq[:-1])\n",
        "    Y.append(seq[-1])\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(X)\n",
        "Y = to_categorical(Y, num_classes=vocab_size)\n",
        "\n",
        "for i in range(len(X)):\n",
        "    print(\"X: {}, Y: {} ({})\".format(X[i], Y[i], tokenizer.index_word[i+1]))  # index assigned to each word starts from 1, not 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhYh3N_eJr2p",
        "outputId": "6b3434ec-a889-4f3e-a1b7-5a7faff0c532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: [0 0 0 0 0 0 0 0 0 0 2], Y: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] (stock)\n",
            "X: [0 0 0 0 0 0 0 0 0 2 1], Y: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] (the)\n",
            "X: [0 0 0 0 0 0 0 0 2 1 3], Y: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] (market)\n",
            "X: [0 0 0 0 0 0 0 2 1 3 4], Y: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] (is)\n",
            "X: [0 0 0 0 0 0 2 1 3 4 5], Y: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] (unpredictable)\n",
            "X: [0 0 0 0 0 2 1 3 4 5 6], Y: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] (but)\n",
            "X: [0 0 0 0 2 1 3 4 5 6 7], Y: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] (has)\n",
            "X: [0 0 0 2 1 3 4 5 6 7 8], Y: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] (trends)\n",
            "X: [0 0 2 1 3 4 5 6 7 8 1], Y: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] (trading)\n",
            "X: [0 2 1 3 4 5 6 7 8 1 9], Y: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] (requires)\n",
            "X: [ 2  1  3  4  5  6  7  8  1  9 10], Y: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] (analysis)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 모델 생성\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 10, input_length=X.shape[1]),\n",
        "    LSTM(50, return_sequences=False),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n"
      ],
      "metadata": {
        "id": "ASyya23oKVma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 전에는 랜덤 단어 출력\n",
        "print(\"다음 단어 예측(훈련 전): 'The stock market is' →\", predict_next_word(model, tokenizer, \"The stock market is\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jplefn_bx7vz",
        "outputId": "8a9949af-d86d-4437-ffd4-f3c01e83cd0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step\n",
            "다음 단어 예측(훈련 전): 'The stock market is' → the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 모델 훈련\n",
        "model.fit(X, Y, epochs=100, verbose=0)\n",
        "\n",
        "# 4. 결과 확인 (다음 단어 예측)\n",
        "def predict_next_word(model, tokenizer, text):\n",
        "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "    sequence = tf.keras.preprocessing.sequence.pad_sequences([sequence], maxlen=X.shape[1])\n",
        "    predicted = model.predict(sequence)\n",
        "    predicted_word = tokenizer.index_word[tf.argmax(predicted, axis=1).numpy()[0]]\n",
        "    return predicted_word\n",
        "\n",
        "print(\"다음 단어 예측(훈련 후): 'The stock market is' →\", predict_next_word(model, tokenizer, \"The stock market is\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vimOPvuix4ox",
        "outputId": "35044815-c7c4-4027-a54b-414110a1c615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "다음 단어 예측(훈련 후): 'The stock market is' → unpredictable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "J3iKp8UHSPnV",
        "outputId": "6a38607f-1aea-4f07-913e-0e3b8d91347a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m10\u001b[0m)                 │             \u001b[38;5;34m120\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m50\u001b[0m)                     │          \u001b[38;5;34m12,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m12\u001b[0m)                     │             \u001b[38;5;34m612\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)                     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">612</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m38,798\u001b[0m (151.56 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">38,798</span> (151.56 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,932\u001b[0m (50.52 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,932</span> (50.52 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m25,866\u001b[0m (101.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,866</span> (101.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 간단한 훈련 예시 2 (문장 생성)\n",
        "- GPT가 어떻게 텍스트를 생성하는지를 간단히 이해시키기 위함 (동작 원리)"
      ],
      "metadata": {
        "id": "4rBLV5L9Q4Ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "# 1. 훈련 데이터 정의\n",
        "data = \"The stock market is unpredictable but has trends. Stock trading requires careful analysis.\"\n",
        "\n",
        "# 2. 토큰화 및 데이터 전처리\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "sequence = tokenizer.texts_to_sequences([data])[0]\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "sequences = []\n",
        "for i in range(1, len(sequence)):\n",
        "    sequences.append(sequence[:i+1])\n",
        "\n",
        "# 입력(X)과 출력(Y) 분리\n",
        "X, Y = [], []\n",
        "for seq in sequences:\n",
        "    X.append(seq[:-1])\n",
        "    Y.append(seq[-1])\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(X)\n",
        "Y = to_categorical(Y, num_classes=vocab_size)\n",
        "\n",
        "# 3. 간단한 모델 정의\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 10, input_length=X.shape[1]),\n",
        "    LSTM(50, return_sequences=False),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "# 4. 모델 학습\n",
        "model.fit(X, Y, epochs=200, verbose=0)\n",
        "\n",
        "# 5. 문장 생성 함수\n",
        "def generate_text(model, tokenizer, seed_text, next_words=10):\n",
        "    for _ in range(next_words):\n",
        "        sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        sequence = tf.keras.preprocessing.sequence.pad_sequences([sequence], maxlen=X.shape[1])\n",
        "        predicted = model.predict(sequence)\n",
        "        predicted_word = tokenizer.index_word[np.argmax(predicted)]\n",
        "        seed_text += \" \" + predicted_word\n",
        "    return seed_text\n",
        "\n",
        "# 6. 결과 확인\n",
        "seed_text = \"The stock market\"\n",
        "generated_text = generate_text(model, tokenizer, seed_text, next_words=10)\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MEFwVYfQ33V",
        "outputId": "3dfaefea-b88c-49c5-dcef-3570f0586088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "Generated Text:\n",
            "The stock market is unpredictable unpredictable has trends stock trading requires careful analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 위의 예제를 통해 알 수 있는 것은 다음과 같다.\n",
        "-  언어모델의 원리:\n",
        "  - 초기 텍스트를 기반으로 문맥에 맞는 단어를 하나씩 예측하여 문장을 확장.\n",
        "- 모델의 한계:\n",
        "  - 단순 데이터로 학습했기 때문에 복잡한 문장 생성이 어려움.\n",
        "  - GPT는 방대한 데이터로 학습해 이 과정을 훨씬 더 정교하게 수행."
      ],
      "metadata": {
        "id": "uGiK8x-oRVsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT구조\n",
        "- Transformer 의 Decoder Stack\n",
        "  - Multi-Head Self-Attention\n",
        "  - Position-Wise Feedforward Network\n",
        "  - Residual Connection 및 Layer Normalization\n",
        "  - Masking: 순방향 정보를 가리기 위해 사용.\n",
        "- 실습\n",
        "  - 작은 크기의 GPT 모델을 구성하고 Attention Score를 시각화"
      ],
      "metadata": {
        "id": "GU2-vjZtzyvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Model, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# 모델 및 토크나이저 로드\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2Model.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Hidden states 활성화\n",
        "model.config.output_hidden_states = True\n",
        "\n",
        "# 입력 처리\n",
        "input_text = \"ChatGPT is an AI model\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "\n",
        "# 모델 실행\n",
        "# outputs = model(**inputs)\n",
        "outputs = model(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
        "\n",
        "# Hidden states 출력\n",
        "hidden_states = outputs.hidden_states\n",
        "print(f\"Number of layers: {len(hidden_states)}\")  # 모든 레이어 출력 개수 확인\n",
        "print(f\"Shape of one layer: {hidden_states[0].shape}\")  # 각 레이어의 텐서 크기\n",
        "\n",
        "print(outputs.last_hidden_state.shape)  # 마지막 레이어의 출력 벡터\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAX97cfD0SvW",
        "outputId": "fbd96870-4969-4b2a-fbd5-4755475b174e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[30820,    38, 11571,   318,   281,  9552,  2746]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
            "Number of layers: 13\n",
            "Shape of one layer: torch.Size([1, 7, 768])\n",
            "torch.Size([1, 7, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyVfIpu90nyN",
        "outputId": "9d9b0701-f0fe-4fb2-b3e6-520c622a2dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[30820,    38, 11571,   318,   281,  9552,  2746]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- attention_mask의 의미\n",
        "  - 동일한 길이의 시퀀스를 처리하도록 패딩된 입력 데이터 위치는 정보가 없는 자리이므로, 모델이 이를 학습하거나 처리하지 않도록 마스킹한다. (1: 해당 위치의 토큰 처리), 0: 해당 위치의 토큰을 무시(마스킹))\n",
        "  - 순방향 정보만을 처리하도록 추가적으로 Causal Masking도 사용 (왼쪽에서 오른쪽으로 순차적인 예측을 수행)"
      ],
      "metadata": {
        "id": "8yWptWquDWCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(inputs['input_ids'][0]))\n",
        "[tokenizer.decode(i) for i in inputs['input_ids'][0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJvOvfBc6AZH",
        "outputId": "7185b40c-7165-4a7a-fb80-c93385daede0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT is an AI model\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Chat', 'G', 'PT', ' is', ' an', ' AI', ' model']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[s.shape for s in outputs.hidden_states]  # 모든 hidden state 들의 shape (batch_size, sequence_length, hidden_size)\n",
        "                                          # hidden_size 는 embedding vector size 임."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhA5VzR7DrCv",
        "outputId": "c6aa47fd-99fb-4a5b-8184-2726d839d2cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([1, 7, 768]),\n",
              " torch.Size([1, 7, 768]),\n",
              " torch.Size([1, 7, 768]),\n",
              " torch.Size([1, 7, 768]),\n",
              " torch.Size([1, 7, 768]),\n",
              " torch.Size([1, 7, 768]),\n",
              " torch.Size([1, 7, 768]),\n",
              " torch.Size([1, 7, 768]),\n",
              " torch.Size([1, 7, 768]),\n",
              " torch.Size([1, 7, 768]),\n",
              " torch.Size([1, 7, 768]),\n",
              " torch.Size([1, 7, 768]),\n",
              " torch.Size([1, 7, 768])]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- hidden_states는 기본적으로 반환되지 않는다.\n",
        "- 반환하려면 model.config.output_hidden_states = True 또는 output_hidden_states=True 옵션을 활성화."
      ],
      "metadata": {
        "id": "Xgh4l6O8Eatf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.last_hidden_state.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6wL9WvdEZMf",
        "outputId": "ef14ebc5-1335-474f-d5ea-59d49f381037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 7, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- outputs.last_hidden_state.shape:\n",
        "  - (1, sequence_length, hidden_size)\n",
        "  - sequence_length는 입력 토큰의 길이, hidden_size는 모델의 차원 크기(예: GPT2의 경우 768)"
      ],
      "metadata": {
        "id": "LUw5FtZEDpT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiwcktMoDNT3",
        "outputId": "8c8f8ad9-d7b1-464b-c439-4ec2bb9add8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Config {\n",
              "  \"_attn_implementation_autoset\": true,\n",
              "  \"_name_or_path\": \"gpt2\",\n",
              "  \"activation_function\": \"gelu_new\",\n",
              "  \"architectures\": [\n",
              "    \"GPT2LMHeadModel\"\n",
              "  ],\n",
              "  \"attn_pdrop\": 0.1,\n",
              "  \"bos_token_id\": 50256,\n",
              "  \"embd_pdrop\": 0.1,\n",
              "  \"eos_token_id\": 50256,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"layer_norm_epsilon\": 1e-05,\n",
              "  \"model_type\": \"gpt2\",\n",
              "  \"n_ctx\": 1024,\n",
              "  \"n_embd\": 768,\n",
              "  \"n_head\": 12,\n",
              "  \"n_inner\": null,\n",
              "  \"n_layer\": 12,\n",
              "  \"n_positions\": 1024,\n",
              "  \"output_hidden_states\": true,\n",
              "  \"reorder_and_upcast_attn\": false,\n",
              "  \"resid_pdrop\": 0.1,\n",
              "  \"scale_attn_by_inverse_layer_idx\": false,\n",
              "  \"scale_attn_weights\": true,\n",
              "  \"summary_activation\": null,\n",
              "  \"summary_first_dropout\": 0.1,\n",
              "  \"summary_proj_to_labels\": true,\n",
              "  \"summary_type\": \"cls_index\",\n",
              "  \"summary_use_proj\": true,\n",
              "  \"task_specific_params\": {\n",
              "    \"text-generation\": {\n",
              "      \"do_sample\": true,\n",
              "      \"max_length\": 50\n",
              "    }\n",
              "  },\n",
              "  \"transformers_version\": \"4.47.1\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50257\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf1cvPH5JucM",
        "outputId": "744e5380-1b9a-43f4-f036-1144412c1455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Model(\n",
            "  (wte): Embedding(50257, 768)\n",
            "  (wpe): Embedding(1024, 768)\n",
            "  (drop): Dropout(p=0.1, inplace=False)\n",
            "  (h): ModuleList(\n",
            "    (0-11): 12 x GPT2Block(\n",
            "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): GPT2SdpaAttention(\n",
            "        (c_attn): Conv1D(nf=2304, nx=768)\n",
            "        (c_proj): Conv1D(nf=768, nx=768)\n",
            "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): GPT2MLP(\n",
            "        (c_fc): Conv1D(nf=3072, nx=768)\n",
            "        (c_proj): Conv1D(nf=768, nx=3072)\n",
            "        (act): NewGELUActivation()\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "    print(f\"Layer: {name}, Type: {type(module)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ-P8sJxJ40F",
        "outputId": "8cbe0f6a-fe4d-46fc-dfe0-0f23af3606ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: , Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2Model'>\n",
            "Layer: wte, Type: <class 'torch.nn.modules.sparse.Embedding'>\n",
            "Layer: wpe, Type: <class 'torch.nn.modules.sparse.Embedding'>\n",
            "Layer: drop, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h, Type: <class 'torch.nn.modules.container.ModuleList'>\n",
            "Layer: h.0, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "Layer: h.0.ln_1, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.0.attn, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
            "Layer: h.0.attn.c_attn, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.0.attn.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.0.attn.attn_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.0.attn.resid_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.0.ln_2, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.0.mlp, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "Layer: h.0.mlp.c_fc, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.0.mlp.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.0.mlp.act, Type: <class 'transformers.activations.NewGELUActivation'>\n",
            "Layer: h.0.mlp.dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.1, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "Layer: h.1.ln_1, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.1.attn, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
            "Layer: h.1.attn.c_attn, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.1.attn.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.1.attn.attn_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.1.attn.resid_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.1.ln_2, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.1.mlp, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "Layer: h.1.mlp.c_fc, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.1.mlp.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.1.mlp.act, Type: <class 'transformers.activations.NewGELUActivation'>\n",
            "Layer: h.1.mlp.dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.2, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "Layer: h.2.ln_1, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.2.attn, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
            "Layer: h.2.attn.c_attn, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.2.attn.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.2.attn.attn_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.2.attn.resid_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.2.ln_2, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.2.mlp, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "Layer: h.2.mlp.c_fc, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.2.mlp.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.2.mlp.act, Type: <class 'transformers.activations.NewGELUActivation'>\n",
            "Layer: h.2.mlp.dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.3, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "Layer: h.3.ln_1, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.3.attn, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
            "Layer: h.3.attn.c_attn, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.3.attn.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.3.attn.attn_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.3.attn.resid_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.3.ln_2, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.3.mlp, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "Layer: h.3.mlp.c_fc, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.3.mlp.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.3.mlp.act, Type: <class 'transformers.activations.NewGELUActivation'>\n",
            "Layer: h.3.mlp.dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.4, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "Layer: h.4.ln_1, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.4.attn, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
            "Layer: h.4.attn.c_attn, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.4.attn.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.4.attn.attn_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.4.attn.resid_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.4.ln_2, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.4.mlp, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "Layer: h.4.mlp.c_fc, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.4.mlp.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.4.mlp.act, Type: <class 'transformers.activations.NewGELUActivation'>\n",
            "Layer: h.4.mlp.dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.5, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "Layer: h.5.ln_1, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.5.attn, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
            "Layer: h.5.attn.c_attn, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.5.attn.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.5.attn.attn_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.5.attn.resid_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.5.ln_2, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.5.mlp, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "Layer: h.5.mlp.c_fc, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.5.mlp.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.5.mlp.act, Type: <class 'transformers.activations.NewGELUActivation'>\n",
            "Layer: h.5.mlp.dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.6, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "Layer: h.6.ln_1, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.6.attn, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
            "Layer: h.6.attn.c_attn, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.6.attn.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.6.attn.attn_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.6.attn.resid_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.6.ln_2, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.6.mlp, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "Layer: h.6.mlp.c_fc, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.6.mlp.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.6.mlp.act, Type: <class 'transformers.activations.NewGELUActivation'>\n",
            "Layer: h.6.mlp.dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.7, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "Layer: h.7.ln_1, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.7.attn, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
            "Layer: h.7.attn.c_attn, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.7.attn.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.7.attn.attn_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.7.attn.resid_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.7.ln_2, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.7.mlp, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "Layer: h.7.mlp.c_fc, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.7.mlp.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.7.mlp.act, Type: <class 'transformers.activations.NewGELUActivation'>\n",
            "Layer: h.7.mlp.dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.8, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "Layer: h.8.ln_1, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.8.attn, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
            "Layer: h.8.attn.c_attn, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.8.attn.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.8.attn.attn_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.8.attn.resid_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.8.ln_2, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.8.mlp, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "Layer: h.8.mlp.c_fc, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.8.mlp.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.8.mlp.act, Type: <class 'transformers.activations.NewGELUActivation'>\n",
            "Layer: h.8.mlp.dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.9, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "Layer: h.9.ln_1, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.9.attn, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
            "Layer: h.9.attn.c_attn, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.9.attn.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.9.attn.attn_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.9.attn.resid_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.9.ln_2, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.9.mlp, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "Layer: h.9.mlp.c_fc, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.9.mlp.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.9.mlp.act, Type: <class 'transformers.activations.NewGELUActivation'>\n",
            "Layer: h.9.mlp.dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.10, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "Layer: h.10.ln_1, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.10.attn, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
            "Layer: h.10.attn.c_attn, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.10.attn.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.10.attn.attn_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.10.attn.resid_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.10.ln_2, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.10.mlp, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "Layer: h.10.mlp.c_fc, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.10.mlp.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.10.mlp.act, Type: <class 'transformers.activations.NewGELUActivation'>\n",
            "Layer: h.10.mlp.dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.11, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n",
            "Layer: h.11.ln_1, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.11.attn, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\n",
            "Layer: h.11.attn.c_attn, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.11.attn.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.11.attn.attn_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.11.attn.resid_dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: h.11.ln_2, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
            "Layer: h.11.mlp, Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n",
            "Layer: h.11.mlp.c_fc, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.11.mlp.c_proj, Type: <class 'transformers.pytorch_utils.Conv1D'>\n",
            "Layer: h.11.mlp.act, Type: <class 'transformers.activations.NewGELUActivation'>\n",
            "Layer: h.11.mlp.dropout, Type: <class 'torch.nn.modules.dropout.Dropout'>\n",
            "Layer: ln_f, Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 시각화"
      ],
      "metadata": {
        "id": "khaXgThXKU4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRMc3Lj7KYhg",
        "outputId": "762e09c9-09ff-44d0-b24b-2d7ebdac44f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.5.1+cu121)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (3.0.2)\n",
            "Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "\n",
        "inputs = tokenizer(\"ChatGPT is amazing\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "make_dot(outputs.last_hidden_state, params=dict(model.named_parameters())).render(\"model_structure\", format=\"png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "odbRRFuKKbo4",
        "outputId": "dfb34e9f-252a-4fd6-a755-d3e9c43a8528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model_structure.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "\n"
      ],
      "metadata": {
        "id": "ykW1v8dTNqAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 토큰 생성"
      ],
      "metadata": {
        "id": "gCobroCi6tzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "texts = [\"Hello, how are you? I am fine, thank you!\"]\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenized_texts = [tokenizer.encode(text) for text in texts]\n",
        "print(tokenized_texts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEnMRQcTN4vZ",
        "outputId": "8257fbcb-ce2e-4b00-fb02-9f5dfedef558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[15496, 11, 703, 389, 345, 30, 314, 716, 3734, 11, 5875, 345, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(tokenized_texts[0]))\n",
        "[tokenizer.decode(i) for i in tokenized_texts[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6XSENh8OXIh",
        "outputId": "af5c8736-41a7-4c26-cb11-290881dcb9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, how are you? I am fine, thank you!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " ',',\n",
              " ' how',\n",
              " ' are',\n",
              " ' you',\n",
              " '?',\n",
              " ' I',\n",
              " ' am',\n",
              " ' fine',\n",
              " ',',\n",
              " ' thank',\n",
              " ' you',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    }
  ]
}